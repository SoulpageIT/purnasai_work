{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kaggle learnings for Diabetic retinopathy competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Were you surprised by any of your findings?**\n",
    "\n",
    "I was surprised by a couple of things. First, that increasing the scale of the images beyond radius=270 pixels did not seem to help. I was expecting the existence of very small features, only visible at higher resolutions, to tip the balance in favor of larger images. Perhaps the increase in processing times for larger images was too great.\n",
    "\n",
    "I was also surprised by the fact that ensembling (taking multiple views of each image, and combining the results of different networks) did very little to improve accuracy. This is rather different to the case of normal photographs, where ensembling can make a huge difference.\n",
    "- **from the Old competition winner [here](http://blog.kaggle.com/2015/09/09/diabetic-retinopathy-winners-interview-1st-place-ben-graham/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What preprocessing and supervised learning methods did you use?**\n",
    "\n",
    "For preprocessing, I first scaled the images to a given radius. I then subtracted local average color to reduce differences in lighting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Techniques by Ben Graham[-Old competition winner-]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://i2.wp.com/blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-10-at-11.12.53-AM.png?w=773\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "print(\"Preprocessing Techniques by Ben Graham[-Old competition winner-]\")\n",
    "Image(url= \"https://i2.wp.com/blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-10-at-11.12.53-AM.png?w=773\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blog on OLD competition winner [here](http://blog.kaggle.com/2015/08/10/detecting-diabetic-retinopathy-in-eye-images/)\n",
    "- Introduction\n",
    "- Overview / TL;DR\n",
    "- The opening (processing and augmenting, kappa metric and first models)\n",
    "- The middlegame (basic architecture, visual attention)\n",
    "- The endgame (camera artifacts, pseudo-labeling, decoding, error distribution, ensembling)\n",
    "- Other (not) tried approaches and papers\n",
    "- Conclusion\n",
    "- Code, models and example activations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**:-\n",
    "Not surprisingly, all my models were convolutional networks (convnets) adapted for this task. I recommend reading the [well-written blogpost with clear explanation](http://benanne.github.io/2015/03/17/plankton.html) from the ≋ Deep Sea ≋ team that won the Kaggle National Data Science Bowl competition since there are a lot of similarities in our approaches and they provide some more/better explanation.\n",
    "                                             \n",
    "  **-- [old competition winner](http://blog.kaggle.com/2015/08/10/detecting-diabetic-retinopathy-in-eye-images/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation techniques from old competition winner\n",
    "These augmentations (transformations) were:\n",
    "\n",
    "1. Cropping with certain probability\n",
    "2. Color balance adjustment\n",
    "3. Brightness adjustment\n",
    "4. Contrast adjustment\n",
    "5. Flipping images with 50% chance\n",
    "6. Rotating images by x degrees, with x an integer in [0, 360[\n",
    "7. Zooming (equal cropping on x and y dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for code, Camera Artifacts, Psuedo LAbelling, Better decoding, Error Distribution Ensembling [here](http://blog.kaggle.com/2015/08/10/detecting-diabetic-retinopathy-in-eye-images/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## know layers and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vgg16 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "           Linear-33                 [-1, 4096]     102,764,544\n",
      "             ReLU-34                 [-1, 4096]               0\n",
      "          Dropout-35                 [-1, 4096]               0\n",
      "           Linear-36                 [-1, 4096]      16,781,312\n",
      "             ReLU-37                 [-1, 4096]               0\n",
      "          Dropout-38                 [-1, 4096]               0\n",
      "           Linear-39                 [-1, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.78\n",
      "Params size (MB): 527.79\n",
      "Estimated Total Size (MB): 747.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device('cpu')\n",
    "#load model\n",
    "vgg = models.vgg16().to(device)\n",
    "#view model or architecture\n",
    "summary(vgg, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Densenet:- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "       BatchNorm2d-5           [-1, 64, 56, 56]             128\n",
      "              ReLU-6           [-1, 64, 56, 56]               0\n",
      "            Conv2d-7          [-1, 128, 56, 56]           8,192\n",
      "       BatchNorm2d-8          [-1, 128, 56, 56]             256\n",
      "              ReLU-9          [-1, 128, 56, 56]               0\n",
      "           Conv2d-10           [-1, 32, 56, 56]          36,864\n",
      "      BatchNorm2d-11           [-1, 96, 56, 56]             192\n",
      "             ReLU-12           [-1, 96, 56, 56]               0\n",
      "           Conv2d-13          [-1, 128, 56, 56]          12,288\n",
      "      BatchNorm2d-14          [-1, 128, 56, 56]             256\n",
      "             ReLU-15          [-1, 128, 56, 56]               0\n",
      "           Conv2d-16           [-1, 32, 56, 56]          36,864\n",
      "      BatchNorm2d-17          [-1, 128, 56, 56]             256\n",
      "             ReLU-18          [-1, 128, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 56, 56]          16,384\n",
      "      BatchNorm2d-20          [-1, 128, 56, 56]             256\n",
      "             ReLU-21          [-1, 128, 56, 56]               0\n",
      "           Conv2d-22           [-1, 32, 56, 56]          36,864\n",
      "      BatchNorm2d-23          [-1, 160, 56, 56]             320\n",
      "             ReLU-24          [-1, 160, 56, 56]               0\n",
      "           Conv2d-25          [-1, 128, 56, 56]          20,480\n",
      "      BatchNorm2d-26          [-1, 128, 56, 56]             256\n",
      "             ReLU-27          [-1, 128, 56, 56]               0\n",
      "           Conv2d-28           [-1, 32, 56, 56]          36,864\n",
      "      BatchNorm2d-29          [-1, 192, 56, 56]             384\n",
      "             ReLU-30          [-1, 192, 56, 56]               0\n",
      "           Conv2d-31          [-1, 128, 56, 56]          24,576\n",
      "      BatchNorm2d-32          [-1, 128, 56, 56]             256\n",
      "             ReLU-33          [-1, 128, 56, 56]               0\n",
      "           Conv2d-34           [-1, 32, 56, 56]          36,864\n",
      "      BatchNorm2d-35          [-1, 224, 56, 56]             448\n",
      "             ReLU-36          [-1, 224, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          28,672\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40           [-1, 32, 56, 56]          36,864\n",
      "      _DenseBlock-41          [-1, 256, 56, 56]               0\n",
      "      BatchNorm2d-42          [-1, 256, 56, 56]             512\n",
      "             ReLU-43          [-1, 256, 56, 56]               0\n",
      "           Conv2d-44          [-1, 128, 56, 56]          32,768\n",
      "        AvgPool2d-45          [-1, 128, 28, 28]               0\n",
      "      BatchNorm2d-46          [-1, 128, 28, 28]             256\n",
      "             ReLU-47          [-1, 128, 28, 28]               0\n",
      "           Conv2d-48          [-1, 128, 28, 28]          16,384\n",
      "      BatchNorm2d-49          [-1, 128, 28, 28]             256\n",
      "             ReLU-50          [-1, 128, 28, 28]               0\n",
      "           Conv2d-51           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-52          [-1, 160, 28, 28]             320\n",
      "             ReLU-53          [-1, 160, 28, 28]               0\n",
      "           Conv2d-54          [-1, 128, 28, 28]          20,480\n",
      "      BatchNorm2d-55          [-1, 128, 28, 28]             256\n",
      "             ReLU-56          [-1, 128, 28, 28]               0\n",
      "           Conv2d-57           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-58          [-1, 192, 28, 28]             384\n",
      "             ReLU-59          [-1, 192, 28, 28]               0\n",
      "           Conv2d-60          [-1, 128, 28, 28]          24,576\n",
      "      BatchNorm2d-61          [-1, 128, 28, 28]             256\n",
      "             ReLU-62          [-1, 128, 28, 28]               0\n",
      "           Conv2d-63           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-64          [-1, 224, 28, 28]             448\n",
      "             ReLU-65          [-1, 224, 28, 28]               0\n",
      "           Conv2d-66          [-1, 128, 28, 28]          28,672\n",
      "      BatchNorm2d-67          [-1, 128, 28, 28]             256\n",
      "             ReLU-68          [-1, 128, 28, 28]               0\n",
      "           Conv2d-69           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-70          [-1, 256, 28, 28]             512\n",
      "             ReLU-71          [-1, 256, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]          32,768\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-76          [-1, 288, 28, 28]             576\n",
      "             ReLU-77          [-1, 288, 28, 28]               0\n",
      "           Conv2d-78          [-1, 128, 28, 28]          36,864\n",
      "      BatchNorm2d-79          [-1, 128, 28, 28]             256\n",
      "             ReLU-80          [-1, 128, 28, 28]               0\n",
      "           Conv2d-81           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-82          [-1, 320, 28, 28]             640\n",
      "             ReLU-83          [-1, 320, 28, 28]               0\n",
      "           Conv2d-84          [-1, 128, 28, 28]          40,960\n",
      "      BatchNorm2d-85          [-1, 128, 28, 28]             256\n",
      "             ReLU-86          [-1, 128, 28, 28]               0\n",
      "           Conv2d-87           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-88          [-1, 352, 28, 28]             704\n",
      "             ReLU-89          [-1, 352, 28, 28]               0\n",
      "           Conv2d-90          [-1, 128, 28, 28]          45,056\n",
      "      BatchNorm2d-91          [-1, 128, 28, 28]             256\n",
      "             ReLU-92          [-1, 128, 28, 28]               0\n",
      "           Conv2d-93           [-1, 32, 28, 28]          36,864\n",
      "      BatchNorm2d-94          [-1, 384, 28, 28]             768\n",
      "             ReLU-95          [-1, 384, 28, 28]               0\n",
      "           Conv2d-96          [-1, 128, 28, 28]          49,152\n",
      "      BatchNorm2d-97          [-1, 128, 28, 28]             256\n",
      "             ReLU-98          [-1, 128, 28, 28]               0\n",
      "           Conv2d-99           [-1, 32, 28, 28]          36,864\n",
      "     BatchNorm2d-100          [-1, 416, 28, 28]             832\n",
      "            ReLU-101          [-1, 416, 28, 28]               0\n",
      "          Conv2d-102          [-1, 128, 28, 28]          53,248\n",
      "     BatchNorm2d-103          [-1, 128, 28, 28]             256\n",
      "            ReLU-104          [-1, 128, 28, 28]               0\n",
      "          Conv2d-105           [-1, 32, 28, 28]          36,864\n",
      "     BatchNorm2d-106          [-1, 448, 28, 28]             896\n",
      "            ReLU-107          [-1, 448, 28, 28]               0\n",
      "          Conv2d-108          [-1, 128, 28, 28]          57,344\n",
      "     BatchNorm2d-109          [-1, 128, 28, 28]             256\n",
      "            ReLU-110          [-1, 128, 28, 28]               0\n",
      "          Conv2d-111           [-1, 32, 28, 28]          36,864\n",
      "     BatchNorm2d-112          [-1, 480, 28, 28]             960\n",
      "            ReLU-113          [-1, 480, 28, 28]               0\n",
      "          Conv2d-114          [-1, 128, 28, 28]          61,440\n",
      "     BatchNorm2d-115          [-1, 128, 28, 28]             256\n",
      "            ReLU-116          [-1, 128, 28, 28]               0\n",
      "          Conv2d-117           [-1, 32, 28, 28]          36,864\n",
      "     _DenseBlock-118          [-1, 512, 28, 28]               0\n",
      "     BatchNorm2d-119          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-120          [-1, 512, 28, 28]               0\n",
      "          Conv2d-121          [-1, 256, 28, 28]         131,072\n",
      "       AvgPool2d-122          [-1, 256, 14, 14]               0\n",
      "     BatchNorm2d-123          [-1, 256, 14, 14]             512\n",
      "            ReLU-124          [-1, 256, 14, 14]               0\n",
      "          Conv2d-125          [-1, 128, 14, 14]          32,768\n",
      "     BatchNorm2d-126          [-1, 128, 14, 14]             256\n",
      "            ReLU-127          [-1, 128, 14, 14]               0\n",
      "          Conv2d-128           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-129          [-1, 288, 14, 14]             576\n",
      "            ReLU-130          [-1, 288, 14, 14]               0\n",
      "          Conv2d-131          [-1, 128, 14, 14]          36,864\n",
      "     BatchNorm2d-132          [-1, 128, 14, 14]             256\n",
      "            ReLU-133          [-1, 128, 14, 14]               0\n",
      "          Conv2d-134           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-135          [-1, 320, 14, 14]             640\n",
      "            ReLU-136          [-1, 320, 14, 14]               0\n",
      "          Conv2d-137          [-1, 128, 14, 14]          40,960\n",
      "     BatchNorm2d-138          [-1, 128, 14, 14]             256\n",
      "            ReLU-139          [-1, 128, 14, 14]               0\n",
      "          Conv2d-140           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-141          [-1, 352, 14, 14]             704\n",
      "            ReLU-142          [-1, 352, 14, 14]               0\n",
      "          Conv2d-143          [-1, 128, 14, 14]          45,056\n",
      "     BatchNorm2d-144          [-1, 128, 14, 14]             256\n",
      "            ReLU-145          [-1, 128, 14, 14]               0\n",
      "          Conv2d-146           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-147          [-1, 384, 14, 14]             768\n",
      "            ReLU-148          [-1, 384, 14, 14]               0\n",
      "          Conv2d-149          [-1, 128, 14, 14]          49,152\n",
      "     BatchNorm2d-150          [-1, 128, 14, 14]             256\n",
      "            ReLU-151          [-1, 128, 14, 14]               0\n",
      "          Conv2d-152           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-153          [-1, 416, 14, 14]             832\n",
      "            ReLU-154          [-1, 416, 14, 14]               0\n",
      "          Conv2d-155          [-1, 128, 14, 14]          53,248\n",
      "     BatchNorm2d-156          [-1, 128, 14, 14]             256\n",
      "            ReLU-157          [-1, 128, 14, 14]               0\n",
      "          Conv2d-158           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-159          [-1, 448, 14, 14]             896\n",
      "            ReLU-160          [-1, 448, 14, 14]               0\n",
      "          Conv2d-161          [-1, 128, 14, 14]          57,344\n",
      "     BatchNorm2d-162          [-1, 128, 14, 14]             256\n",
      "            ReLU-163          [-1, 128, 14, 14]               0\n",
      "          Conv2d-164           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-165          [-1, 480, 14, 14]             960\n",
      "            ReLU-166          [-1, 480, 14, 14]               0\n",
      "          Conv2d-167          [-1, 128, 14, 14]          61,440\n",
      "     BatchNorm2d-168          [-1, 128, 14, 14]             256\n",
      "            ReLU-169          [-1, 128, 14, 14]               0\n",
      "          Conv2d-170           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-171          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-172          [-1, 512, 14, 14]               0\n",
      "          Conv2d-173          [-1, 128, 14, 14]          65,536\n",
      "     BatchNorm2d-174          [-1, 128, 14, 14]             256\n",
      "            ReLU-175          [-1, 128, 14, 14]               0\n",
      "          Conv2d-176           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-177          [-1, 544, 14, 14]           1,088\n",
      "            ReLU-178          [-1, 544, 14, 14]               0\n",
      "          Conv2d-179          [-1, 128, 14, 14]          69,632\n",
      "     BatchNorm2d-180          [-1, 128, 14, 14]             256\n",
      "            ReLU-181          [-1, 128, 14, 14]               0\n",
      "          Conv2d-182           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-183          [-1, 576, 14, 14]           1,152\n",
      "            ReLU-184          [-1, 576, 14, 14]               0\n",
      "          Conv2d-185          [-1, 128, 14, 14]          73,728\n",
      "     BatchNorm2d-186          [-1, 128, 14, 14]             256\n",
      "            ReLU-187          [-1, 128, 14, 14]               0\n",
      "          Conv2d-188           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-189          [-1, 608, 14, 14]           1,216\n",
      "            ReLU-190          [-1, 608, 14, 14]               0\n",
      "          Conv2d-191          [-1, 128, 14, 14]          77,824\n",
      "     BatchNorm2d-192          [-1, 128, 14, 14]             256\n",
      "            ReLU-193          [-1, 128, 14, 14]               0\n",
      "          Conv2d-194           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-195          [-1, 640, 14, 14]           1,280\n",
      "            ReLU-196          [-1, 640, 14, 14]               0\n",
      "          Conv2d-197          [-1, 128, 14, 14]          81,920\n",
      "     BatchNorm2d-198          [-1, 128, 14, 14]             256\n",
      "            ReLU-199          [-1, 128, 14, 14]               0\n",
      "          Conv2d-200           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-201          [-1, 672, 14, 14]           1,344\n",
      "            ReLU-202          [-1, 672, 14, 14]               0\n",
      "          Conv2d-203          [-1, 128, 14, 14]          86,016\n",
      "     BatchNorm2d-204          [-1, 128, 14, 14]             256\n",
      "            ReLU-205          [-1, 128, 14, 14]               0\n",
      "          Conv2d-206           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-207          [-1, 704, 14, 14]           1,408\n",
      "            ReLU-208          [-1, 704, 14, 14]               0\n",
      "          Conv2d-209          [-1, 128, 14, 14]          90,112\n",
      "     BatchNorm2d-210          [-1, 128, 14, 14]             256\n",
      "            ReLU-211          [-1, 128, 14, 14]               0\n",
      "          Conv2d-212           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-213          [-1, 736, 14, 14]           1,472\n",
      "            ReLU-214          [-1, 736, 14, 14]               0\n",
      "          Conv2d-215          [-1, 128, 14, 14]          94,208\n",
      "     BatchNorm2d-216          [-1, 128, 14, 14]             256\n",
      "            ReLU-217          [-1, 128, 14, 14]               0\n",
      "          Conv2d-218           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-219          [-1, 768, 14, 14]           1,536\n",
      "            ReLU-220          [-1, 768, 14, 14]               0\n",
      "          Conv2d-221          [-1, 128, 14, 14]          98,304\n",
      "     BatchNorm2d-222          [-1, 128, 14, 14]             256\n",
      "            ReLU-223          [-1, 128, 14, 14]               0\n",
      "          Conv2d-224           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-225          [-1, 800, 14, 14]           1,600\n",
      "            ReLU-226          [-1, 800, 14, 14]               0\n",
      "          Conv2d-227          [-1, 128, 14, 14]         102,400\n",
      "     BatchNorm2d-228          [-1, 128, 14, 14]             256\n",
      "            ReLU-229          [-1, 128, 14, 14]               0\n",
      "          Conv2d-230           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-231          [-1, 832, 14, 14]           1,664\n",
      "            ReLU-232          [-1, 832, 14, 14]               0\n",
      "          Conv2d-233          [-1, 128, 14, 14]         106,496\n",
      "     BatchNorm2d-234          [-1, 128, 14, 14]             256\n",
      "            ReLU-235          [-1, 128, 14, 14]               0\n",
      "          Conv2d-236           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-237          [-1, 864, 14, 14]           1,728\n",
      "            ReLU-238          [-1, 864, 14, 14]               0\n",
      "          Conv2d-239          [-1, 128, 14, 14]         110,592\n",
      "     BatchNorm2d-240          [-1, 128, 14, 14]             256\n",
      "            ReLU-241          [-1, 128, 14, 14]               0\n",
      "          Conv2d-242           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-243          [-1, 896, 14, 14]           1,792\n",
      "            ReLU-244          [-1, 896, 14, 14]               0\n",
      "          Conv2d-245          [-1, 128, 14, 14]         114,688\n",
      "     BatchNorm2d-246          [-1, 128, 14, 14]             256\n",
      "            ReLU-247          [-1, 128, 14, 14]               0\n",
      "          Conv2d-248           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-249          [-1, 928, 14, 14]           1,856\n",
      "            ReLU-250          [-1, 928, 14, 14]               0\n",
      "          Conv2d-251          [-1, 128, 14, 14]         118,784\n",
      "     BatchNorm2d-252          [-1, 128, 14, 14]             256\n",
      "            ReLU-253          [-1, 128, 14, 14]               0\n",
      "          Conv2d-254           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-255          [-1, 960, 14, 14]           1,920\n",
      "            ReLU-256          [-1, 960, 14, 14]               0\n",
      "          Conv2d-257          [-1, 128, 14, 14]         122,880\n",
      "     BatchNorm2d-258          [-1, 128, 14, 14]             256\n",
      "            ReLU-259          [-1, 128, 14, 14]               0\n",
      "          Conv2d-260           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-261          [-1, 992, 14, 14]           1,984\n",
      "            ReLU-262          [-1, 992, 14, 14]               0\n",
      "          Conv2d-263          [-1, 128, 14, 14]         126,976\n",
      "     BatchNorm2d-264          [-1, 128, 14, 14]             256\n",
      "            ReLU-265          [-1, 128, 14, 14]               0\n",
      "          Conv2d-266           [-1, 32, 14, 14]          36,864\n",
      "     _DenseBlock-267         [-1, 1024, 14, 14]               0\n",
      "     BatchNorm2d-268         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-269         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-270          [-1, 512, 14, 14]         524,288\n",
      "       AvgPool2d-271            [-1, 512, 7, 7]               0\n",
      "     BatchNorm2d-272            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-273            [-1, 512, 7, 7]               0\n",
      "          Conv2d-274            [-1, 128, 7, 7]          65,536\n",
      "     BatchNorm2d-275            [-1, 128, 7, 7]             256\n",
      "            ReLU-276            [-1, 128, 7, 7]               0\n",
      "          Conv2d-277             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-278            [-1, 544, 7, 7]           1,088\n",
      "            ReLU-279            [-1, 544, 7, 7]               0\n",
      "          Conv2d-280            [-1, 128, 7, 7]          69,632\n",
      "     BatchNorm2d-281            [-1, 128, 7, 7]             256\n",
      "            ReLU-282            [-1, 128, 7, 7]               0\n",
      "          Conv2d-283             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-284            [-1, 576, 7, 7]           1,152\n",
      "            ReLU-285            [-1, 576, 7, 7]               0\n",
      "          Conv2d-286            [-1, 128, 7, 7]          73,728\n",
      "     BatchNorm2d-287            [-1, 128, 7, 7]             256\n",
      "            ReLU-288            [-1, 128, 7, 7]               0\n",
      "          Conv2d-289             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-290            [-1, 608, 7, 7]           1,216\n",
      "            ReLU-291            [-1, 608, 7, 7]               0\n",
      "          Conv2d-292            [-1, 128, 7, 7]          77,824\n",
      "     BatchNorm2d-293            [-1, 128, 7, 7]             256\n",
      "            ReLU-294            [-1, 128, 7, 7]               0\n",
      "          Conv2d-295             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-296            [-1, 640, 7, 7]           1,280\n",
      "            ReLU-297            [-1, 640, 7, 7]               0\n",
      "          Conv2d-298            [-1, 128, 7, 7]          81,920\n",
      "     BatchNorm2d-299            [-1, 128, 7, 7]             256\n",
      "            ReLU-300            [-1, 128, 7, 7]               0\n",
      "          Conv2d-301             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-302            [-1, 672, 7, 7]           1,344\n",
      "            ReLU-303            [-1, 672, 7, 7]               0\n",
      "          Conv2d-304            [-1, 128, 7, 7]          86,016\n",
      "     BatchNorm2d-305            [-1, 128, 7, 7]             256\n",
      "            ReLU-306            [-1, 128, 7, 7]               0\n",
      "          Conv2d-307             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-308            [-1, 704, 7, 7]           1,408\n",
      "            ReLU-309            [-1, 704, 7, 7]               0\n",
      "          Conv2d-310            [-1, 128, 7, 7]          90,112\n",
      "     BatchNorm2d-311            [-1, 128, 7, 7]             256\n",
      "            ReLU-312            [-1, 128, 7, 7]               0\n",
      "          Conv2d-313             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-314            [-1, 736, 7, 7]           1,472\n",
      "            ReLU-315            [-1, 736, 7, 7]               0\n",
      "          Conv2d-316            [-1, 128, 7, 7]          94,208\n",
      "     BatchNorm2d-317            [-1, 128, 7, 7]             256\n",
      "            ReLU-318            [-1, 128, 7, 7]               0\n",
      "          Conv2d-319             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-320            [-1, 768, 7, 7]           1,536\n",
      "            ReLU-321            [-1, 768, 7, 7]               0\n",
      "          Conv2d-322            [-1, 128, 7, 7]          98,304\n",
      "     BatchNorm2d-323            [-1, 128, 7, 7]             256\n",
      "            ReLU-324            [-1, 128, 7, 7]               0\n",
      "          Conv2d-325             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-326            [-1, 800, 7, 7]           1,600\n",
      "            ReLU-327            [-1, 800, 7, 7]               0\n",
      "          Conv2d-328            [-1, 128, 7, 7]         102,400\n",
      "     BatchNorm2d-329            [-1, 128, 7, 7]             256\n",
      "            ReLU-330            [-1, 128, 7, 7]               0\n",
      "          Conv2d-331             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-332            [-1, 832, 7, 7]           1,664\n",
      "            ReLU-333            [-1, 832, 7, 7]               0\n",
      "          Conv2d-334            [-1, 128, 7, 7]         106,496\n",
      "     BatchNorm2d-335            [-1, 128, 7, 7]             256\n",
      "            ReLU-336            [-1, 128, 7, 7]               0\n",
      "          Conv2d-337             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-338            [-1, 864, 7, 7]           1,728\n",
      "            ReLU-339            [-1, 864, 7, 7]               0\n",
      "          Conv2d-340            [-1, 128, 7, 7]         110,592\n",
      "     BatchNorm2d-341            [-1, 128, 7, 7]             256\n",
      "            ReLU-342            [-1, 128, 7, 7]               0\n",
      "          Conv2d-343             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-344            [-1, 896, 7, 7]           1,792\n",
      "            ReLU-345            [-1, 896, 7, 7]               0\n",
      "          Conv2d-346            [-1, 128, 7, 7]         114,688\n",
      "     BatchNorm2d-347            [-1, 128, 7, 7]             256\n",
      "            ReLU-348            [-1, 128, 7, 7]               0\n",
      "          Conv2d-349             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-350            [-1, 928, 7, 7]           1,856\n",
      "            ReLU-351            [-1, 928, 7, 7]               0\n",
      "          Conv2d-352            [-1, 128, 7, 7]         118,784\n",
      "     BatchNorm2d-353            [-1, 128, 7, 7]             256\n",
      "            ReLU-354            [-1, 128, 7, 7]               0\n",
      "          Conv2d-355             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-356            [-1, 960, 7, 7]           1,920\n",
      "            ReLU-357            [-1, 960, 7, 7]               0\n",
      "          Conv2d-358            [-1, 128, 7, 7]         122,880\n",
      "     BatchNorm2d-359            [-1, 128, 7, 7]             256\n",
      "            ReLU-360            [-1, 128, 7, 7]               0\n",
      "          Conv2d-361             [-1, 32, 7, 7]          36,864\n",
      "     BatchNorm2d-362            [-1, 992, 7, 7]           1,984\n",
      "            ReLU-363            [-1, 992, 7, 7]               0\n",
      "          Conv2d-364            [-1, 128, 7, 7]         126,976\n",
      "     BatchNorm2d-365            [-1, 128, 7, 7]             256\n",
      "            ReLU-366            [-1, 128, 7, 7]               0\n",
      "          Conv2d-367             [-1, 32, 7, 7]          36,864\n",
      "     _DenseBlock-368           [-1, 1024, 7, 7]               0\n",
      "     BatchNorm2d-369           [-1, 1024, 7, 7]           2,048\n",
      "          Linear-370                 [-1, 1000]       1,025,000\n",
      "================================================================\n",
      "Total params: 7,978,856\n",
      "Trainable params: 7,978,856\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 305.30\n",
      "Params size (MB): 30.44\n",
      "Estimated Total Size (MB): 336.31\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device('cpu')\n",
    "#load model\n",
    "dense = models.densenet121().to(device)\n",
    "#view model or architecture\n",
    "summary(dense, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-142          [-1, 256, 14, 14]             512\n",
      "            ReLU-143          [-1, 256, 14, 14]               0\n",
      "          Conv2d-144          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-145          [-1, 256, 14, 14]             512\n",
      "            ReLU-146          [-1, 256, 14, 14]               0\n",
      "          Conv2d-147         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-148         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-149         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-150         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-151          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-152          [-1, 256, 14, 14]             512\n",
      "            ReLU-153          [-1, 256, 14, 14]               0\n",
      "          Conv2d-154          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-155          [-1, 256, 14, 14]             512\n",
      "            ReLU-156          [-1, 256, 14, 14]               0\n",
      "          Conv2d-157         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-158         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-159         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-160         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-161          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-162          [-1, 256, 14, 14]             512\n",
      "            ReLU-163          [-1, 256, 14, 14]               0\n",
      "          Conv2d-164          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-165          [-1, 256, 14, 14]             512\n",
      "            ReLU-166          [-1, 256, 14, 14]               0\n",
      "          Conv2d-167         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-168         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-169         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-170         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-171          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-172          [-1, 256, 14, 14]             512\n",
      "            ReLU-173          [-1, 256, 14, 14]               0\n",
      "          Conv2d-174          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-175          [-1, 256, 14, 14]             512\n",
      "            ReLU-176          [-1, 256, 14, 14]               0\n",
      "          Conv2d-177         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-178         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-179         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-180         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-181          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-182          [-1, 256, 14, 14]             512\n",
      "            ReLU-183          [-1, 256, 14, 14]               0\n",
      "          Conv2d-184          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-185          [-1, 256, 14, 14]             512\n",
      "            ReLU-186          [-1, 256, 14, 14]               0\n",
      "          Conv2d-187         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-188         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-189         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-190         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-191          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-192          [-1, 256, 14, 14]             512\n",
      "            ReLU-193          [-1, 256, 14, 14]               0\n",
      "          Conv2d-194          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-195          [-1, 256, 14, 14]             512\n",
      "            ReLU-196          [-1, 256, 14, 14]               0\n",
      "          Conv2d-197         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-198         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-199         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-200         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-201          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-202          [-1, 256, 14, 14]             512\n",
      "            ReLU-203          [-1, 256, 14, 14]               0\n",
      "          Conv2d-204          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-205          [-1, 256, 14, 14]             512\n",
      "            ReLU-206          [-1, 256, 14, 14]               0\n",
      "          Conv2d-207         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-208         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-209         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-210         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-211          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-212          [-1, 256, 14, 14]             512\n",
      "            ReLU-213          [-1, 256, 14, 14]               0\n",
      "          Conv2d-214          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-215          [-1, 256, 14, 14]             512\n",
      "            ReLU-216          [-1, 256, 14, 14]               0\n",
      "          Conv2d-217         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-218         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-219         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-220         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-221          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-222          [-1, 256, 14, 14]             512\n",
      "            ReLU-223          [-1, 256, 14, 14]               0\n",
      "          Conv2d-224          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-225          [-1, 256, 14, 14]             512\n",
      "            ReLU-226          [-1, 256, 14, 14]               0\n",
      "          Conv2d-227         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-228         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-229         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-230         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-231          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-232          [-1, 256, 14, 14]             512\n",
      "            ReLU-233          [-1, 256, 14, 14]               0\n",
      "          Conv2d-234          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-235          [-1, 256, 14, 14]             512\n",
      "            ReLU-236          [-1, 256, 14, 14]               0\n",
      "          Conv2d-237         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-238         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-239         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-240         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-241          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-242          [-1, 256, 14, 14]             512\n",
      "            ReLU-243          [-1, 256, 14, 14]               0\n",
      "          Conv2d-244          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-245          [-1, 256, 14, 14]             512\n",
      "            ReLU-246          [-1, 256, 14, 14]               0\n",
      "          Conv2d-247         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-248         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-249         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-250         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-251          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-252          [-1, 256, 14, 14]             512\n",
      "            ReLU-253          [-1, 256, 14, 14]               0\n",
      "          Conv2d-254          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-255          [-1, 256, 14, 14]             512\n",
      "            ReLU-256          [-1, 256, 14, 14]               0\n",
      "          Conv2d-257         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-258         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-259         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-260         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-261          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-262          [-1, 256, 14, 14]             512\n",
      "            ReLU-263          [-1, 256, 14, 14]               0\n",
      "          Conv2d-264          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-265          [-1, 256, 14, 14]             512\n",
      "            ReLU-266          [-1, 256, 14, 14]               0\n",
      "          Conv2d-267         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-268         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-269         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-270         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-271          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-272          [-1, 256, 14, 14]             512\n",
      "            ReLU-273          [-1, 256, 14, 14]               0\n",
      "          Conv2d-274          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-275          [-1, 256, 14, 14]             512\n",
      "            ReLU-276          [-1, 256, 14, 14]               0\n",
      "          Conv2d-277         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-278         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-279         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-280         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-281          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-282          [-1, 256, 14, 14]             512\n",
      "            ReLU-283          [-1, 256, 14, 14]               0\n",
      "          Conv2d-284          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-285          [-1, 256, 14, 14]             512\n",
      "            ReLU-286          [-1, 256, 14, 14]               0\n",
      "          Conv2d-287         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-288         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-289         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-290         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-291          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-292          [-1, 256, 14, 14]             512\n",
      "            ReLU-293          [-1, 256, 14, 14]               0\n",
      "          Conv2d-294          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-295          [-1, 256, 14, 14]             512\n",
      "            ReLU-296          [-1, 256, 14, 14]               0\n",
      "          Conv2d-297         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-298         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-299         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-300         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-301          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-302          [-1, 256, 14, 14]             512\n",
      "            ReLU-303          [-1, 256, 14, 14]               0\n",
      "          Conv2d-304          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-305          [-1, 256, 14, 14]             512\n",
      "            ReLU-306          [-1, 256, 14, 14]               0\n",
      "          Conv2d-307         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-308         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-309         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-310         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-311          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-312          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-313          [-1, 512, 14, 14]               0\n",
      "          Conv2d-314            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-315            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-316            [-1, 512, 7, 7]               0\n",
      "          Conv2d-317           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-318           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-319           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-320           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-321           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-322           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-323            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-324            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-325            [-1, 512, 7, 7]               0\n",
      "          Conv2d-326            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-327            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-328            [-1, 512, 7, 7]               0\n",
      "          Conv2d-329           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-330           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-331           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-332           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-333            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-334            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-335            [-1, 512, 7, 7]               0\n",
      "          Conv2d-336            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-337            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-338            [-1, 512, 7, 7]               0\n",
      "          Conv2d-339           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-340           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-341           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-342           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-343           [-1, 2048, 1, 1]               0\n",
      "          Linear-344                 [-1, 1000]       2,049,000\n",
      "================================================================\n",
      "Total params: 44,549,160\n",
      "Trainable params: 44,549,160\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 429.73\n",
      "Params size (MB): 169.94\n",
      "Estimated Total Size (MB): 600.25\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device('cpu')\n",
    "#load model\n",
    "dense = models.resnet101().to(device)\n",
    "#view model or architecture\n",
    "summary(dense, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ResNet' object has no attribute 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2454c6f16370>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'conv'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    589\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 591\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ResNet' object has no attribute 'layers'"
     ]
    }
   ],
   "source": [
    "for layer in dense.layers:\n",
    "    print(layer.name)\n",
    "    if 'conv' not in layer.name:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Official link to verify sensitivity and specificity](https://www.eyediagnosis.co/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding sensitivity and specificity\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.healthnewsreview.org/wp-content/uploads/2018/06/sensitivity-2.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "print(\"Understanding sensitivity and specificity\")\n",
    "Image(url= \"https://www.healthnewsreview.org/wp-content/uploads/2018/06/sensitivity-2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above fig is from the link [here](https://www.healthnewsreview.org/toolkit/tips-for-understanding-studies/understanding-medical-tests-sensitivity-specificity-and-positive-predictive-value/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Visualzing Densenet using pytorch](http://www.andrewjanowczyk.com/visualizing-densenet-using-pytorch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Understanding and visualzing Densenet](https://towardsdatascience.com/understanding-and-visualizing-densenets-7f688092391a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### optimization of predictions using You can get best coefficients using nelder-mead optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [optimizer for quadratic weighted kappa](https://www.kaggle.com/abhishek/optimizer-for-quadratic-weighted-kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to do that?\n",
    "\n",
    "Well, if your regression results range from 0 to infinity, you can say everything below 0.5 is class 0, between 0.5 and 1.5 is class 1, between 1.5 and 2.5 is class 2, between 2.3 and 3.5 is class 3 and everything above 3.5 is class 4.\n",
    "\n",
    "Thus the coefficients are : [0.5, 1.5, 2.5, 3.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more of above from **[here](https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/76107)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learnt how to apply QWK from [here](https://www.kaggle.com/venkat555/aptos-pytorch-starter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding sensitivity and specificity\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/tensorflow/tpu/master/models/official/efficientnet/g3doc/params.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "print(\"Models with loss and parameters\")\n",
    "Image(url= \"https://raw.githubusercontent.com/tensorflow/tpu/master/models/official/efficientnet/g3doc/params.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above fig is from [here](https://www.kaggle.com/carlolepelaars/efficientnetb5-with-keras-aptos-2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get layer and its weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "conv1.weight \t torch.Size([64, 3, 7, 7])\n",
      "bn1.weight \t torch.Size([64])\n",
      "bn1.bias \t torch.Size([64])\n",
      "bn1.running_mean \t torch.Size([64])\n",
      "bn1.running_var \t torch.Size([64])\n",
      "bn1.num_batches_tracked \t torch.Size([])\n",
      "layer1.0.conv1.weight \t torch.Size([64, 64, 1, 1])\n",
      "layer1.0.bn1.weight \t torch.Size([64])\n",
      "layer1.0.bn1.bias \t torch.Size([64])\n",
      "layer1.0.bn1.running_mean \t torch.Size([64])\n",
      "layer1.0.bn1.running_var \t torch.Size([64])\n",
      "layer1.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer1.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn2.weight \t torch.Size([64])\n",
      "layer1.0.bn2.bias \t torch.Size([64])\n",
      "layer1.0.bn2.running_mean \t torch.Size([64])\n",
      "layer1.0.bn2.running_var \t torch.Size([64])\n",
      "layer1.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer1.0.conv3.weight \t torch.Size([256, 64, 1, 1])\n",
      "layer1.0.bn3.weight \t torch.Size([256])\n",
      "layer1.0.bn3.bias \t torch.Size([256])\n",
      "layer1.0.bn3.running_mean \t torch.Size([256])\n",
      "layer1.0.bn3.running_var \t torch.Size([256])\n",
      "layer1.0.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer1.0.downsample.0.weight \t torch.Size([256, 64, 1, 1])\n",
      "layer1.0.downsample.1.weight \t torch.Size([256])\n",
      "layer1.0.downsample.1.bias \t torch.Size([256])\n",
      "layer1.0.downsample.1.running_mean \t torch.Size([256])\n",
      "layer1.0.downsample.1.running_var \t torch.Size([256])\n",
      "layer1.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "layer1.1.conv1.weight \t torch.Size([64, 256, 1, 1])\n",
      "layer1.1.bn1.weight \t torch.Size([64])\n",
      "layer1.1.bn1.bias \t torch.Size([64])\n",
      "layer1.1.bn1.running_mean \t torch.Size([64])\n",
      "layer1.1.bn1.running_var \t torch.Size([64])\n",
      "layer1.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer1.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn2.weight \t torch.Size([64])\n",
      "layer1.1.bn2.bias \t torch.Size([64])\n",
      "layer1.1.bn2.running_mean \t torch.Size([64])\n",
      "layer1.1.bn2.running_var \t torch.Size([64])\n",
      "layer1.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer1.1.conv3.weight \t torch.Size([256, 64, 1, 1])\n",
      "layer1.1.bn3.weight \t torch.Size([256])\n",
      "layer1.1.bn3.bias \t torch.Size([256])\n",
      "layer1.1.bn3.running_mean \t torch.Size([256])\n",
      "layer1.1.bn3.running_var \t torch.Size([256])\n",
      "layer1.1.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer1.2.conv1.weight \t torch.Size([64, 256, 1, 1])\n",
      "layer1.2.bn1.weight \t torch.Size([64])\n",
      "layer1.2.bn1.bias \t torch.Size([64])\n",
      "layer1.2.bn1.running_mean \t torch.Size([64])\n",
      "layer1.2.bn1.running_var \t torch.Size([64])\n",
      "layer1.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer1.2.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.2.bn2.weight \t torch.Size([64])\n",
      "layer1.2.bn2.bias \t torch.Size([64])\n",
      "layer1.2.bn2.running_mean \t torch.Size([64])\n",
      "layer1.2.bn2.running_var \t torch.Size([64])\n",
      "layer1.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer1.2.conv3.weight \t torch.Size([256, 64, 1, 1])\n",
      "layer1.2.bn3.weight \t torch.Size([256])\n",
      "layer1.2.bn3.bias \t torch.Size([256])\n",
      "layer1.2.bn3.running_mean \t torch.Size([256])\n",
      "layer1.2.bn3.running_var \t torch.Size([256])\n",
      "layer1.2.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.0.conv1.weight \t torch.Size([128, 256, 1, 1])\n",
      "layer2.0.bn1.weight \t torch.Size([128])\n",
      "layer2.0.bn1.bias \t torch.Size([128])\n",
      "layer2.0.bn1.running_mean \t torch.Size([128])\n",
      "layer2.0.bn1.running_var \t torch.Size([128])\n",
      "layer2.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.0.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.0.bn2.weight \t torch.Size([128])\n",
      "layer2.0.bn2.bias \t torch.Size([128])\n",
      "layer2.0.bn2.running_mean \t torch.Size([128])\n",
      "layer2.0.bn2.running_var \t torch.Size([128])\n",
      "layer2.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.0.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.0.bn3.weight \t torch.Size([512])\n",
      "layer2.0.bn3.bias \t torch.Size([512])\n",
      "layer2.0.bn3.running_mean \t torch.Size([512])\n",
      "layer2.0.bn3.running_var \t torch.Size([512])\n",
      "layer2.0.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.0.downsample.0.weight \t torch.Size([512, 256, 1, 1])\n",
      "layer2.0.downsample.1.weight \t torch.Size([512])\n",
      "layer2.0.downsample.1.bias \t torch.Size([512])\n",
      "layer2.0.downsample.1.running_mean \t torch.Size([512])\n",
      "layer2.0.downsample.1.running_var \t torch.Size([512])\n",
      "layer2.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "layer2.1.conv1.weight \t torch.Size([128, 512, 1, 1])\n",
      "layer2.1.bn1.weight \t torch.Size([128])\n",
      "layer2.1.bn1.bias \t torch.Size([128])\n",
      "layer2.1.bn1.running_mean \t torch.Size([128])\n",
      "layer2.1.bn1.running_var \t torch.Size([128])\n",
      "layer2.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.1.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn2.weight \t torch.Size([128])\n",
      "layer2.1.bn2.bias \t torch.Size([128])\n",
      "layer2.1.bn2.running_mean \t torch.Size([128])\n",
      "layer2.1.bn2.running_var \t torch.Size([128])\n",
      "layer2.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.1.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.1.bn3.weight \t torch.Size([512])\n",
      "layer2.1.bn3.bias \t torch.Size([512])\n",
      "layer2.1.bn3.running_mean \t torch.Size([512])\n",
      "layer2.1.bn3.running_var \t torch.Size([512])\n",
      "layer2.1.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.2.conv1.weight \t torch.Size([128, 512, 1, 1])\n",
      "layer2.2.bn1.weight \t torch.Size([128])\n",
      "layer2.2.bn1.bias \t torch.Size([128])\n",
      "layer2.2.bn1.running_mean \t torch.Size([128])\n",
      "layer2.2.bn1.running_var \t torch.Size([128])\n",
      "layer2.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.2.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.2.bn2.weight \t torch.Size([128])\n",
      "layer2.2.bn2.bias \t torch.Size([128])\n",
      "layer2.2.bn2.running_mean \t torch.Size([128])\n",
      "layer2.2.bn2.running_var \t torch.Size([128])\n",
      "layer2.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.2.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.2.bn3.weight \t torch.Size([512])\n",
      "layer2.2.bn3.bias \t torch.Size([512])\n",
      "layer2.2.bn3.running_mean \t torch.Size([512])\n",
      "layer2.2.bn3.running_var \t torch.Size([512])\n",
      "layer2.2.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.3.conv1.weight \t torch.Size([128, 512, 1, 1])\n",
      "layer2.3.bn1.weight \t torch.Size([128])\n",
      "layer2.3.bn1.bias \t torch.Size([128])\n",
      "layer2.3.bn1.running_mean \t torch.Size([128])\n",
      "layer2.3.bn1.running_var \t torch.Size([128])\n",
      "layer2.3.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.3.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.3.bn2.weight \t torch.Size([128])\n",
      "layer2.3.bn2.bias \t torch.Size([128])\n",
      "layer2.3.bn2.running_mean \t torch.Size([128])\n",
      "layer2.3.bn2.running_var \t torch.Size([128])\n",
      "layer2.3.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.3.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.3.bn3.weight \t torch.Size([512])\n",
      "layer2.3.bn3.bias \t torch.Size([512])\n",
      "layer2.3.bn3.running_mean \t torch.Size([512])\n",
      "layer2.3.bn3.running_var \t torch.Size([512])\n",
      "layer2.3.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.4.conv1.weight \t torch.Size([128, 512, 1, 1])\n",
      "layer2.4.bn1.weight \t torch.Size([128])\n",
      "layer2.4.bn1.bias \t torch.Size([128])\n",
      "layer2.4.bn1.running_mean \t torch.Size([128])\n",
      "layer2.4.bn1.running_var \t torch.Size([128])\n",
      "layer2.4.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.4.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.4.bn2.weight \t torch.Size([128])\n",
      "layer2.4.bn2.bias \t torch.Size([128])\n",
      "layer2.4.bn2.running_mean \t torch.Size([128])\n",
      "layer2.4.bn2.running_var \t torch.Size([128])\n",
      "layer2.4.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.4.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.4.bn3.weight \t torch.Size([512])\n",
      "layer2.4.bn3.bias \t torch.Size([512])\n",
      "layer2.4.bn3.running_mean \t torch.Size([512])\n",
      "layer2.4.bn3.running_var \t torch.Size([512])\n",
      "layer2.4.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.5.conv1.weight \t torch.Size([128, 512, 1, 1])\n",
      "layer2.5.bn1.weight \t torch.Size([128])\n",
      "layer2.5.bn1.bias \t torch.Size([128])\n",
      "layer2.5.bn1.running_mean \t torch.Size([128])\n",
      "layer2.5.bn1.running_var \t torch.Size([128])\n",
      "layer2.5.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.5.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.5.bn2.weight \t torch.Size([128])\n",
      "layer2.5.bn2.bias \t torch.Size([128])\n",
      "layer2.5.bn2.running_mean \t torch.Size([128])\n",
      "layer2.5.bn2.running_var \t torch.Size([128])\n",
      "layer2.5.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.5.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.5.bn3.weight \t torch.Size([512])\n",
      "layer2.5.bn3.bias \t torch.Size([512])\n",
      "layer2.5.bn3.running_mean \t torch.Size([512])\n",
      "layer2.5.bn3.running_var \t torch.Size([512])\n",
      "layer2.5.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.6.conv1.weight \t torch.Size([128, 512, 1, 1])\n",
      "layer2.6.bn1.weight \t torch.Size([128])\n",
      "layer2.6.bn1.bias \t torch.Size([128])\n",
      "layer2.6.bn1.running_mean \t torch.Size([128])\n",
      "layer2.6.bn1.running_var \t torch.Size([128])\n",
      "layer2.6.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.6.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.6.bn2.weight \t torch.Size([128])\n",
      "layer2.6.bn2.bias \t torch.Size([128])\n",
      "layer2.6.bn2.running_mean \t torch.Size([128])\n",
      "layer2.6.bn2.running_var \t torch.Size([128])\n",
      "layer2.6.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.6.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.6.bn3.weight \t torch.Size([512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer2.6.bn3.bias \t torch.Size([512])\n",
      "layer2.6.bn3.running_mean \t torch.Size([512])\n",
      "layer2.6.bn3.running_var \t torch.Size([512])\n",
      "layer2.6.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.7.conv1.weight \t torch.Size([128, 512, 1, 1])\n",
      "layer2.7.bn1.weight \t torch.Size([128])\n",
      "layer2.7.bn1.bias \t torch.Size([128])\n",
      "layer2.7.bn1.running_mean \t torch.Size([128])\n",
      "layer2.7.bn1.running_var \t torch.Size([128])\n",
      "layer2.7.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.7.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.7.bn2.weight \t torch.Size([128])\n",
      "layer2.7.bn2.bias \t torch.Size([128])\n",
      "layer2.7.bn2.running_mean \t torch.Size([128])\n",
      "layer2.7.bn2.running_var \t torch.Size([128])\n",
      "layer2.7.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.7.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.7.bn3.weight \t torch.Size([512])\n",
      "layer2.7.bn3.bias \t torch.Size([512])\n",
      "layer2.7.bn3.running_mean \t torch.Size([512])\n",
      "layer2.7.bn3.running_var \t torch.Size([512])\n",
      "layer2.7.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.0.conv1.weight \t torch.Size([256, 512, 1, 1])\n",
      "layer3.0.bn1.weight \t torch.Size([256])\n",
      "layer3.0.bn1.bias \t torch.Size([256])\n",
      "layer3.0.bn1.running_mean \t torch.Size([256])\n",
      "layer3.0.bn1.running_var \t torch.Size([256])\n",
      "layer3.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.0.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.0.bn2.weight \t torch.Size([256])\n",
      "layer3.0.bn2.bias \t torch.Size([256])\n",
      "layer3.0.bn2.running_mean \t torch.Size([256])\n",
      "layer3.0.bn2.running_var \t torch.Size([256])\n",
      "layer3.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.0.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.0.bn3.weight \t torch.Size([1024])\n",
      "layer3.0.bn3.bias \t torch.Size([1024])\n",
      "layer3.0.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.0.bn3.running_var \t torch.Size([1024])\n",
      "layer3.0.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.0.downsample.0.weight \t torch.Size([1024, 512, 1, 1])\n",
      "layer3.0.downsample.1.weight \t torch.Size([1024])\n",
      "layer3.0.downsample.1.bias \t torch.Size([1024])\n",
      "layer3.0.downsample.1.running_mean \t torch.Size([1024])\n",
      "layer3.0.downsample.1.running_var \t torch.Size([1024])\n",
      "layer3.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "layer3.1.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.1.bn1.weight \t torch.Size([256])\n",
      "layer3.1.bn1.bias \t torch.Size([256])\n",
      "layer3.1.bn1.running_mean \t torch.Size([256])\n",
      "layer3.1.bn1.running_var \t torch.Size([256])\n",
      "layer3.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.1.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn2.weight \t torch.Size([256])\n",
      "layer3.1.bn2.bias \t torch.Size([256])\n",
      "layer3.1.bn2.running_mean \t torch.Size([256])\n",
      "layer3.1.bn2.running_var \t torch.Size([256])\n",
      "layer3.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.1.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.1.bn3.weight \t torch.Size([1024])\n",
      "layer3.1.bn3.bias \t torch.Size([1024])\n",
      "layer3.1.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.1.bn3.running_var \t torch.Size([1024])\n",
      "layer3.1.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.2.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.2.bn1.weight \t torch.Size([256])\n",
      "layer3.2.bn1.bias \t torch.Size([256])\n",
      "layer3.2.bn1.running_mean \t torch.Size([256])\n",
      "layer3.2.bn1.running_var \t torch.Size([256])\n",
      "layer3.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.2.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.2.bn2.weight \t torch.Size([256])\n",
      "layer3.2.bn2.bias \t torch.Size([256])\n",
      "layer3.2.bn2.running_mean \t torch.Size([256])\n",
      "layer3.2.bn2.running_var \t torch.Size([256])\n",
      "layer3.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.2.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.2.bn3.weight \t torch.Size([1024])\n",
      "layer3.2.bn3.bias \t torch.Size([1024])\n",
      "layer3.2.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.2.bn3.running_var \t torch.Size([1024])\n",
      "layer3.2.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.3.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.3.bn1.weight \t torch.Size([256])\n",
      "layer3.3.bn1.bias \t torch.Size([256])\n",
      "layer3.3.bn1.running_mean \t torch.Size([256])\n",
      "layer3.3.bn1.running_var \t torch.Size([256])\n",
      "layer3.3.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.3.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.3.bn2.weight \t torch.Size([256])\n",
      "layer3.3.bn2.bias \t torch.Size([256])\n",
      "layer3.3.bn2.running_mean \t torch.Size([256])\n",
      "layer3.3.bn2.running_var \t torch.Size([256])\n",
      "layer3.3.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.3.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.3.bn3.weight \t torch.Size([1024])\n",
      "layer3.3.bn3.bias \t torch.Size([1024])\n",
      "layer3.3.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.3.bn3.running_var \t torch.Size([1024])\n",
      "layer3.3.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.4.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.4.bn1.weight \t torch.Size([256])\n",
      "layer3.4.bn1.bias \t torch.Size([256])\n",
      "layer3.4.bn1.running_mean \t torch.Size([256])\n",
      "layer3.4.bn1.running_var \t torch.Size([256])\n",
      "layer3.4.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.4.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.4.bn2.weight \t torch.Size([256])\n",
      "layer3.4.bn2.bias \t torch.Size([256])\n",
      "layer3.4.bn2.running_mean \t torch.Size([256])\n",
      "layer3.4.bn2.running_var \t torch.Size([256])\n",
      "layer3.4.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.4.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.4.bn3.weight \t torch.Size([1024])\n",
      "layer3.4.bn3.bias \t torch.Size([1024])\n",
      "layer3.4.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.4.bn3.running_var \t torch.Size([1024])\n",
      "layer3.4.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.5.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.5.bn1.weight \t torch.Size([256])\n",
      "layer3.5.bn1.bias \t torch.Size([256])\n",
      "layer3.5.bn1.running_mean \t torch.Size([256])\n",
      "layer3.5.bn1.running_var \t torch.Size([256])\n",
      "layer3.5.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.5.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.5.bn2.weight \t torch.Size([256])\n",
      "layer3.5.bn2.bias \t torch.Size([256])\n",
      "layer3.5.bn2.running_mean \t torch.Size([256])\n",
      "layer3.5.bn2.running_var \t torch.Size([256])\n",
      "layer3.5.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.5.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.5.bn3.weight \t torch.Size([1024])\n",
      "layer3.5.bn3.bias \t torch.Size([1024])\n",
      "layer3.5.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.5.bn3.running_var \t torch.Size([1024])\n",
      "layer3.5.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.6.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.6.bn1.weight \t torch.Size([256])\n",
      "layer3.6.bn1.bias \t torch.Size([256])\n",
      "layer3.6.bn1.running_mean \t torch.Size([256])\n",
      "layer3.6.bn1.running_var \t torch.Size([256])\n",
      "layer3.6.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.6.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.6.bn2.weight \t torch.Size([256])\n",
      "layer3.6.bn2.bias \t torch.Size([256])\n",
      "layer3.6.bn2.running_mean \t torch.Size([256])\n",
      "layer3.6.bn2.running_var \t torch.Size([256])\n",
      "layer3.6.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.6.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.6.bn3.weight \t torch.Size([1024])\n",
      "layer3.6.bn3.bias \t torch.Size([1024])\n",
      "layer3.6.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.6.bn3.running_var \t torch.Size([1024])\n",
      "layer3.6.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.7.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.7.bn1.weight \t torch.Size([256])\n",
      "layer3.7.bn1.bias \t torch.Size([256])\n",
      "layer3.7.bn1.running_mean \t torch.Size([256])\n",
      "layer3.7.bn1.running_var \t torch.Size([256])\n",
      "layer3.7.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.7.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.7.bn2.weight \t torch.Size([256])\n",
      "layer3.7.bn2.bias \t torch.Size([256])\n",
      "layer3.7.bn2.running_mean \t torch.Size([256])\n",
      "layer3.7.bn2.running_var \t torch.Size([256])\n",
      "layer3.7.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.7.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.7.bn3.weight \t torch.Size([1024])\n",
      "layer3.7.bn3.bias \t torch.Size([1024])\n",
      "layer3.7.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.7.bn3.running_var \t torch.Size([1024])\n",
      "layer3.7.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.8.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.8.bn1.weight \t torch.Size([256])\n",
      "layer3.8.bn1.bias \t torch.Size([256])\n",
      "layer3.8.bn1.running_mean \t torch.Size([256])\n",
      "layer3.8.bn1.running_var \t torch.Size([256])\n",
      "layer3.8.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.8.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.8.bn2.weight \t torch.Size([256])\n",
      "layer3.8.bn2.bias \t torch.Size([256])\n",
      "layer3.8.bn2.running_mean \t torch.Size([256])\n",
      "layer3.8.bn2.running_var \t torch.Size([256])\n",
      "layer3.8.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.8.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.8.bn3.weight \t torch.Size([1024])\n",
      "layer3.8.bn3.bias \t torch.Size([1024])\n",
      "layer3.8.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.8.bn3.running_var \t torch.Size([1024])\n",
      "layer3.8.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.9.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.9.bn1.weight \t torch.Size([256])\n",
      "layer3.9.bn1.bias \t torch.Size([256])\n",
      "layer3.9.bn1.running_mean \t torch.Size([256])\n",
      "layer3.9.bn1.running_var \t torch.Size([256])\n",
      "layer3.9.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.9.conv2.weight \t torch.Size([256, 256, 3, 3])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer3.9.bn2.weight \t torch.Size([256])\n",
      "layer3.9.bn2.bias \t torch.Size([256])\n",
      "layer3.9.bn2.running_mean \t torch.Size([256])\n",
      "layer3.9.bn2.running_var \t torch.Size([256])\n",
      "layer3.9.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.9.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.9.bn3.weight \t torch.Size([1024])\n",
      "layer3.9.bn3.bias \t torch.Size([1024])\n",
      "layer3.9.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.9.bn3.running_var \t torch.Size([1024])\n",
      "layer3.9.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.10.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.10.bn1.weight \t torch.Size([256])\n",
      "layer3.10.bn1.bias \t torch.Size([256])\n",
      "layer3.10.bn1.running_mean \t torch.Size([256])\n",
      "layer3.10.bn1.running_var \t torch.Size([256])\n",
      "layer3.10.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.10.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.10.bn2.weight \t torch.Size([256])\n",
      "layer3.10.bn2.bias \t torch.Size([256])\n",
      "layer3.10.bn2.running_mean \t torch.Size([256])\n",
      "layer3.10.bn2.running_var \t torch.Size([256])\n",
      "layer3.10.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.10.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.10.bn3.weight \t torch.Size([1024])\n",
      "layer3.10.bn3.bias \t torch.Size([1024])\n",
      "layer3.10.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.10.bn3.running_var \t torch.Size([1024])\n",
      "layer3.10.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.11.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.11.bn1.weight \t torch.Size([256])\n",
      "layer3.11.bn1.bias \t torch.Size([256])\n",
      "layer3.11.bn1.running_mean \t torch.Size([256])\n",
      "layer3.11.bn1.running_var \t torch.Size([256])\n",
      "layer3.11.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.11.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.11.bn2.weight \t torch.Size([256])\n",
      "layer3.11.bn2.bias \t torch.Size([256])\n",
      "layer3.11.bn2.running_mean \t torch.Size([256])\n",
      "layer3.11.bn2.running_var \t torch.Size([256])\n",
      "layer3.11.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.11.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.11.bn3.weight \t torch.Size([1024])\n",
      "layer3.11.bn3.bias \t torch.Size([1024])\n",
      "layer3.11.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.11.bn3.running_var \t torch.Size([1024])\n",
      "layer3.11.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.12.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.12.bn1.weight \t torch.Size([256])\n",
      "layer3.12.bn1.bias \t torch.Size([256])\n",
      "layer3.12.bn1.running_mean \t torch.Size([256])\n",
      "layer3.12.bn1.running_var \t torch.Size([256])\n",
      "layer3.12.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.12.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.12.bn2.weight \t torch.Size([256])\n",
      "layer3.12.bn2.bias \t torch.Size([256])\n",
      "layer3.12.bn2.running_mean \t torch.Size([256])\n",
      "layer3.12.bn2.running_var \t torch.Size([256])\n",
      "layer3.12.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.12.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.12.bn3.weight \t torch.Size([1024])\n",
      "layer3.12.bn3.bias \t torch.Size([1024])\n",
      "layer3.12.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.12.bn3.running_var \t torch.Size([1024])\n",
      "layer3.12.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.13.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.13.bn1.weight \t torch.Size([256])\n",
      "layer3.13.bn1.bias \t torch.Size([256])\n",
      "layer3.13.bn1.running_mean \t torch.Size([256])\n",
      "layer3.13.bn1.running_var \t torch.Size([256])\n",
      "layer3.13.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.13.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.13.bn2.weight \t torch.Size([256])\n",
      "layer3.13.bn2.bias \t torch.Size([256])\n",
      "layer3.13.bn2.running_mean \t torch.Size([256])\n",
      "layer3.13.bn2.running_var \t torch.Size([256])\n",
      "layer3.13.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.13.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.13.bn3.weight \t torch.Size([1024])\n",
      "layer3.13.bn3.bias \t torch.Size([1024])\n",
      "layer3.13.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.13.bn3.running_var \t torch.Size([1024])\n",
      "layer3.13.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.14.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.14.bn1.weight \t torch.Size([256])\n",
      "layer3.14.bn1.bias \t torch.Size([256])\n",
      "layer3.14.bn1.running_mean \t torch.Size([256])\n",
      "layer3.14.bn1.running_var \t torch.Size([256])\n",
      "layer3.14.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.14.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.14.bn2.weight \t torch.Size([256])\n",
      "layer3.14.bn2.bias \t torch.Size([256])\n",
      "layer3.14.bn2.running_mean \t torch.Size([256])\n",
      "layer3.14.bn2.running_var \t torch.Size([256])\n",
      "layer3.14.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.14.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.14.bn3.weight \t torch.Size([1024])\n",
      "layer3.14.bn3.bias \t torch.Size([1024])\n",
      "layer3.14.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.14.bn3.running_var \t torch.Size([1024])\n",
      "layer3.14.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.15.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.15.bn1.weight \t torch.Size([256])\n",
      "layer3.15.bn1.bias \t torch.Size([256])\n",
      "layer3.15.bn1.running_mean \t torch.Size([256])\n",
      "layer3.15.bn1.running_var \t torch.Size([256])\n",
      "layer3.15.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.15.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.15.bn2.weight \t torch.Size([256])\n",
      "layer3.15.bn2.bias \t torch.Size([256])\n",
      "layer3.15.bn2.running_mean \t torch.Size([256])\n",
      "layer3.15.bn2.running_var \t torch.Size([256])\n",
      "layer3.15.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.15.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.15.bn3.weight \t torch.Size([1024])\n",
      "layer3.15.bn3.bias \t torch.Size([1024])\n",
      "layer3.15.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.15.bn3.running_var \t torch.Size([1024])\n",
      "layer3.15.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.16.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.16.bn1.weight \t torch.Size([256])\n",
      "layer3.16.bn1.bias \t torch.Size([256])\n",
      "layer3.16.bn1.running_mean \t torch.Size([256])\n",
      "layer3.16.bn1.running_var \t torch.Size([256])\n",
      "layer3.16.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.16.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.16.bn2.weight \t torch.Size([256])\n",
      "layer3.16.bn2.bias \t torch.Size([256])\n",
      "layer3.16.bn2.running_mean \t torch.Size([256])\n",
      "layer3.16.bn2.running_var \t torch.Size([256])\n",
      "layer3.16.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.16.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.16.bn3.weight \t torch.Size([1024])\n",
      "layer3.16.bn3.bias \t torch.Size([1024])\n",
      "layer3.16.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.16.bn3.running_var \t torch.Size([1024])\n",
      "layer3.16.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.17.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.17.bn1.weight \t torch.Size([256])\n",
      "layer3.17.bn1.bias \t torch.Size([256])\n",
      "layer3.17.bn1.running_mean \t torch.Size([256])\n",
      "layer3.17.bn1.running_var \t torch.Size([256])\n",
      "layer3.17.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.17.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.17.bn2.weight \t torch.Size([256])\n",
      "layer3.17.bn2.bias \t torch.Size([256])\n",
      "layer3.17.bn2.running_mean \t torch.Size([256])\n",
      "layer3.17.bn2.running_var \t torch.Size([256])\n",
      "layer3.17.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.17.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.17.bn3.weight \t torch.Size([1024])\n",
      "layer3.17.bn3.bias \t torch.Size([1024])\n",
      "layer3.17.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.17.bn3.running_var \t torch.Size([1024])\n",
      "layer3.17.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.18.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.18.bn1.weight \t torch.Size([256])\n",
      "layer3.18.bn1.bias \t torch.Size([256])\n",
      "layer3.18.bn1.running_mean \t torch.Size([256])\n",
      "layer3.18.bn1.running_var \t torch.Size([256])\n",
      "layer3.18.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.18.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.18.bn2.weight \t torch.Size([256])\n",
      "layer3.18.bn2.bias \t torch.Size([256])\n",
      "layer3.18.bn2.running_mean \t torch.Size([256])\n",
      "layer3.18.bn2.running_var \t torch.Size([256])\n",
      "layer3.18.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.18.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.18.bn3.weight \t torch.Size([1024])\n",
      "layer3.18.bn3.bias \t torch.Size([1024])\n",
      "layer3.18.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.18.bn3.running_var \t torch.Size([1024])\n",
      "layer3.18.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.19.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.19.bn1.weight \t torch.Size([256])\n",
      "layer3.19.bn1.bias \t torch.Size([256])\n",
      "layer3.19.bn1.running_mean \t torch.Size([256])\n",
      "layer3.19.bn1.running_var \t torch.Size([256])\n",
      "layer3.19.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.19.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.19.bn2.weight \t torch.Size([256])\n",
      "layer3.19.bn2.bias \t torch.Size([256])\n",
      "layer3.19.bn2.running_mean \t torch.Size([256])\n",
      "layer3.19.bn2.running_var \t torch.Size([256])\n",
      "layer3.19.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.19.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.19.bn3.weight \t torch.Size([1024])\n",
      "layer3.19.bn3.bias \t torch.Size([1024])\n",
      "layer3.19.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.19.bn3.running_var \t torch.Size([1024])\n",
      "layer3.19.bn3.num_batches_tracked \t torch.Size([])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer3.20.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.20.bn1.weight \t torch.Size([256])\n",
      "layer3.20.bn1.bias \t torch.Size([256])\n",
      "layer3.20.bn1.running_mean \t torch.Size([256])\n",
      "layer3.20.bn1.running_var \t torch.Size([256])\n",
      "layer3.20.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.20.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.20.bn2.weight \t torch.Size([256])\n",
      "layer3.20.bn2.bias \t torch.Size([256])\n",
      "layer3.20.bn2.running_mean \t torch.Size([256])\n",
      "layer3.20.bn2.running_var \t torch.Size([256])\n",
      "layer3.20.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.20.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.20.bn3.weight \t torch.Size([1024])\n",
      "layer3.20.bn3.bias \t torch.Size([1024])\n",
      "layer3.20.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.20.bn3.running_var \t torch.Size([1024])\n",
      "layer3.20.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.21.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.21.bn1.weight \t torch.Size([256])\n",
      "layer3.21.bn1.bias \t torch.Size([256])\n",
      "layer3.21.bn1.running_mean \t torch.Size([256])\n",
      "layer3.21.bn1.running_var \t torch.Size([256])\n",
      "layer3.21.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.21.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.21.bn2.weight \t torch.Size([256])\n",
      "layer3.21.bn2.bias \t torch.Size([256])\n",
      "layer3.21.bn2.running_mean \t torch.Size([256])\n",
      "layer3.21.bn2.running_var \t torch.Size([256])\n",
      "layer3.21.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.21.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.21.bn3.weight \t torch.Size([1024])\n",
      "layer3.21.bn3.bias \t torch.Size([1024])\n",
      "layer3.21.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.21.bn3.running_var \t torch.Size([1024])\n",
      "layer3.21.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.22.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.22.bn1.weight \t torch.Size([256])\n",
      "layer3.22.bn1.bias \t torch.Size([256])\n",
      "layer3.22.bn1.running_mean \t torch.Size([256])\n",
      "layer3.22.bn1.running_var \t torch.Size([256])\n",
      "layer3.22.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.22.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.22.bn2.weight \t torch.Size([256])\n",
      "layer3.22.bn2.bias \t torch.Size([256])\n",
      "layer3.22.bn2.running_mean \t torch.Size([256])\n",
      "layer3.22.bn2.running_var \t torch.Size([256])\n",
      "layer3.22.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.22.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.22.bn3.weight \t torch.Size([1024])\n",
      "layer3.22.bn3.bias \t torch.Size([1024])\n",
      "layer3.22.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.22.bn3.running_var \t torch.Size([1024])\n",
      "layer3.22.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.23.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.23.bn1.weight \t torch.Size([256])\n",
      "layer3.23.bn1.bias \t torch.Size([256])\n",
      "layer3.23.bn1.running_mean \t torch.Size([256])\n",
      "layer3.23.bn1.running_var \t torch.Size([256])\n",
      "layer3.23.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.23.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.23.bn2.weight \t torch.Size([256])\n",
      "layer3.23.bn2.bias \t torch.Size([256])\n",
      "layer3.23.bn2.running_mean \t torch.Size([256])\n",
      "layer3.23.bn2.running_var \t torch.Size([256])\n",
      "layer3.23.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.23.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.23.bn3.weight \t torch.Size([1024])\n",
      "layer3.23.bn3.bias \t torch.Size([1024])\n",
      "layer3.23.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.23.bn3.running_var \t torch.Size([1024])\n",
      "layer3.23.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.24.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.24.bn1.weight \t torch.Size([256])\n",
      "layer3.24.bn1.bias \t torch.Size([256])\n",
      "layer3.24.bn1.running_mean \t torch.Size([256])\n",
      "layer3.24.bn1.running_var \t torch.Size([256])\n",
      "layer3.24.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.24.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.24.bn2.weight \t torch.Size([256])\n",
      "layer3.24.bn2.bias \t torch.Size([256])\n",
      "layer3.24.bn2.running_mean \t torch.Size([256])\n",
      "layer3.24.bn2.running_var \t torch.Size([256])\n",
      "layer3.24.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.24.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.24.bn3.weight \t torch.Size([1024])\n",
      "layer3.24.bn3.bias \t torch.Size([1024])\n",
      "layer3.24.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.24.bn3.running_var \t torch.Size([1024])\n",
      "layer3.24.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.25.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.25.bn1.weight \t torch.Size([256])\n",
      "layer3.25.bn1.bias \t torch.Size([256])\n",
      "layer3.25.bn1.running_mean \t torch.Size([256])\n",
      "layer3.25.bn1.running_var \t torch.Size([256])\n",
      "layer3.25.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.25.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.25.bn2.weight \t torch.Size([256])\n",
      "layer3.25.bn2.bias \t torch.Size([256])\n",
      "layer3.25.bn2.running_mean \t torch.Size([256])\n",
      "layer3.25.bn2.running_var \t torch.Size([256])\n",
      "layer3.25.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.25.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.25.bn3.weight \t torch.Size([1024])\n",
      "layer3.25.bn3.bias \t torch.Size([1024])\n",
      "layer3.25.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.25.bn3.running_var \t torch.Size([1024])\n",
      "layer3.25.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.26.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.26.bn1.weight \t torch.Size([256])\n",
      "layer3.26.bn1.bias \t torch.Size([256])\n",
      "layer3.26.bn1.running_mean \t torch.Size([256])\n",
      "layer3.26.bn1.running_var \t torch.Size([256])\n",
      "layer3.26.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.26.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.26.bn2.weight \t torch.Size([256])\n",
      "layer3.26.bn2.bias \t torch.Size([256])\n",
      "layer3.26.bn2.running_mean \t torch.Size([256])\n",
      "layer3.26.bn2.running_var \t torch.Size([256])\n",
      "layer3.26.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.26.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.26.bn3.weight \t torch.Size([1024])\n",
      "layer3.26.bn3.bias \t torch.Size([1024])\n",
      "layer3.26.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.26.bn3.running_var \t torch.Size([1024])\n",
      "layer3.26.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.27.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.27.bn1.weight \t torch.Size([256])\n",
      "layer3.27.bn1.bias \t torch.Size([256])\n",
      "layer3.27.bn1.running_mean \t torch.Size([256])\n",
      "layer3.27.bn1.running_var \t torch.Size([256])\n",
      "layer3.27.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.27.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.27.bn2.weight \t torch.Size([256])\n",
      "layer3.27.bn2.bias \t torch.Size([256])\n",
      "layer3.27.bn2.running_mean \t torch.Size([256])\n",
      "layer3.27.bn2.running_var \t torch.Size([256])\n",
      "layer3.27.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.27.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.27.bn3.weight \t torch.Size([1024])\n",
      "layer3.27.bn3.bias \t torch.Size([1024])\n",
      "layer3.27.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.27.bn3.running_var \t torch.Size([1024])\n",
      "layer3.27.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.28.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.28.bn1.weight \t torch.Size([256])\n",
      "layer3.28.bn1.bias \t torch.Size([256])\n",
      "layer3.28.bn1.running_mean \t torch.Size([256])\n",
      "layer3.28.bn1.running_var \t torch.Size([256])\n",
      "layer3.28.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.28.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.28.bn2.weight \t torch.Size([256])\n",
      "layer3.28.bn2.bias \t torch.Size([256])\n",
      "layer3.28.bn2.running_mean \t torch.Size([256])\n",
      "layer3.28.bn2.running_var \t torch.Size([256])\n",
      "layer3.28.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.28.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.28.bn3.weight \t torch.Size([1024])\n",
      "layer3.28.bn3.bias \t torch.Size([1024])\n",
      "layer3.28.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.28.bn3.running_var \t torch.Size([1024])\n",
      "layer3.28.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.29.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.29.bn1.weight \t torch.Size([256])\n",
      "layer3.29.bn1.bias \t torch.Size([256])\n",
      "layer3.29.bn1.running_mean \t torch.Size([256])\n",
      "layer3.29.bn1.running_var \t torch.Size([256])\n",
      "layer3.29.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.29.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.29.bn2.weight \t torch.Size([256])\n",
      "layer3.29.bn2.bias \t torch.Size([256])\n",
      "layer3.29.bn2.running_mean \t torch.Size([256])\n",
      "layer3.29.bn2.running_var \t torch.Size([256])\n",
      "layer3.29.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.29.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.29.bn3.weight \t torch.Size([1024])\n",
      "layer3.29.bn3.bias \t torch.Size([1024])\n",
      "layer3.29.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.29.bn3.running_var \t torch.Size([1024])\n",
      "layer3.29.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.30.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.30.bn1.weight \t torch.Size([256])\n",
      "layer3.30.bn1.bias \t torch.Size([256])\n",
      "layer3.30.bn1.running_mean \t torch.Size([256])\n",
      "layer3.30.bn1.running_var \t torch.Size([256])\n",
      "layer3.30.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.30.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.30.bn2.weight \t torch.Size([256])\n",
      "layer3.30.bn2.bias \t torch.Size([256])\n",
      "layer3.30.bn2.running_mean \t torch.Size([256])\n",
      "layer3.30.bn2.running_var \t torch.Size([256])\n",
      "layer3.30.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.30.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.30.bn3.weight \t torch.Size([1024])\n",
      "layer3.30.bn3.bias \t torch.Size([1024])\n",
      "layer3.30.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.30.bn3.running_var \t torch.Size([1024])\n",
      "layer3.30.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.31.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.31.bn1.weight \t torch.Size([256])\n",
      "layer3.31.bn1.bias \t torch.Size([256])\n",
      "layer3.31.bn1.running_mean \t torch.Size([256])\n",
      "layer3.31.bn1.running_var \t torch.Size([256])\n",
      "layer3.31.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.31.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.31.bn2.weight \t torch.Size([256])\n",
      "layer3.31.bn2.bias \t torch.Size([256])\n",
      "layer3.31.bn2.running_mean \t torch.Size([256])\n",
      "layer3.31.bn2.running_var \t torch.Size([256])\n",
      "layer3.31.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.31.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.31.bn3.weight \t torch.Size([1024])\n",
      "layer3.31.bn3.bias \t torch.Size([1024])\n",
      "layer3.31.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.31.bn3.running_var \t torch.Size([1024])\n",
      "layer3.31.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.32.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.32.bn1.weight \t torch.Size([256])\n",
      "layer3.32.bn1.bias \t torch.Size([256])\n",
      "layer3.32.bn1.running_mean \t torch.Size([256])\n",
      "layer3.32.bn1.running_var \t torch.Size([256])\n",
      "layer3.32.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.32.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.32.bn2.weight \t torch.Size([256])\n",
      "layer3.32.bn2.bias \t torch.Size([256])\n",
      "layer3.32.bn2.running_mean \t torch.Size([256])\n",
      "layer3.32.bn2.running_var \t torch.Size([256])\n",
      "layer3.32.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.32.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.32.bn3.weight \t torch.Size([1024])\n",
      "layer3.32.bn3.bias \t torch.Size([1024])\n",
      "layer3.32.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.32.bn3.running_var \t torch.Size([1024])\n",
      "layer3.32.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.33.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.33.bn1.weight \t torch.Size([256])\n",
      "layer3.33.bn1.bias \t torch.Size([256])\n",
      "layer3.33.bn1.running_mean \t torch.Size([256])\n",
      "layer3.33.bn1.running_var \t torch.Size([256])\n",
      "layer3.33.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.33.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.33.bn2.weight \t torch.Size([256])\n",
      "layer3.33.bn2.bias \t torch.Size([256])\n",
      "layer3.33.bn2.running_mean \t torch.Size([256])\n",
      "layer3.33.bn2.running_var \t torch.Size([256])\n",
      "layer3.33.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.33.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.33.bn3.weight \t torch.Size([1024])\n",
      "layer3.33.bn3.bias \t torch.Size([1024])\n",
      "layer3.33.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.33.bn3.running_var \t torch.Size([1024])\n",
      "layer3.33.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.34.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.34.bn1.weight \t torch.Size([256])\n",
      "layer3.34.bn1.bias \t torch.Size([256])\n",
      "layer3.34.bn1.running_mean \t torch.Size([256])\n",
      "layer3.34.bn1.running_var \t torch.Size([256])\n",
      "layer3.34.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.34.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.34.bn2.weight \t torch.Size([256])\n",
      "layer3.34.bn2.bias \t torch.Size([256])\n",
      "layer3.34.bn2.running_mean \t torch.Size([256])\n",
      "layer3.34.bn2.running_var \t torch.Size([256])\n",
      "layer3.34.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.34.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.34.bn3.weight \t torch.Size([1024])\n",
      "layer3.34.bn3.bias \t torch.Size([1024])\n",
      "layer3.34.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.34.bn3.running_var \t torch.Size([1024])\n",
      "layer3.34.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.35.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.35.bn1.weight \t torch.Size([256])\n",
      "layer3.35.bn1.bias \t torch.Size([256])\n",
      "layer3.35.bn1.running_mean \t torch.Size([256])\n",
      "layer3.35.bn1.running_var \t torch.Size([256])\n",
      "layer3.35.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.35.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.35.bn2.weight \t torch.Size([256])\n",
      "layer3.35.bn2.bias \t torch.Size([256])\n",
      "layer3.35.bn2.running_mean \t torch.Size([256])\n",
      "layer3.35.bn2.running_var \t torch.Size([256])\n",
      "layer3.35.bn2.num_batches_tracked \t torch.Size([])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer3.35.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.35.bn3.weight \t torch.Size([1024])\n",
      "layer3.35.bn3.bias \t torch.Size([1024])\n",
      "layer3.35.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.35.bn3.running_var \t torch.Size([1024])\n",
      "layer3.35.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer4.0.conv1.weight \t torch.Size([512, 1024, 1, 1])\n",
      "layer4.0.bn1.weight \t torch.Size([512])\n",
      "layer4.0.bn1.bias \t torch.Size([512])\n",
      "layer4.0.bn1.running_mean \t torch.Size([512])\n",
      "layer4.0.bn1.running_var \t torch.Size([512])\n",
      "layer4.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer4.0.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4.0.bn2.weight \t torch.Size([512])\n",
      "layer4.0.bn2.bias \t torch.Size([512])\n",
      "layer4.0.bn2.running_mean \t torch.Size([512])\n",
      "layer4.0.bn2.running_var \t torch.Size([512])\n",
      "layer4.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer4.0.conv3.weight \t torch.Size([2048, 512, 1, 1])\n",
      "layer4.0.bn3.weight \t torch.Size([2048])\n",
      "layer4.0.bn3.bias \t torch.Size([2048])\n",
      "layer4.0.bn3.running_mean \t torch.Size([2048])\n",
      "layer4.0.bn3.running_var \t torch.Size([2048])\n",
      "layer4.0.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer4.0.downsample.0.weight \t torch.Size([2048, 1024, 1, 1])\n",
      "layer4.0.downsample.1.weight \t torch.Size([2048])\n",
      "layer4.0.downsample.1.bias \t torch.Size([2048])\n",
      "layer4.0.downsample.1.running_mean \t torch.Size([2048])\n",
      "layer4.0.downsample.1.running_var \t torch.Size([2048])\n",
      "layer4.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "layer4.1.conv1.weight \t torch.Size([512, 2048, 1, 1])\n",
      "layer4.1.bn1.weight \t torch.Size([512])\n",
      "layer4.1.bn1.bias \t torch.Size([512])\n",
      "layer4.1.bn1.running_mean \t torch.Size([512])\n",
      "layer4.1.bn1.running_var \t torch.Size([512])\n",
      "layer4.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer4.1.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4.1.bn2.weight \t torch.Size([512])\n",
      "layer4.1.bn2.bias \t torch.Size([512])\n",
      "layer4.1.bn2.running_mean \t torch.Size([512])\n",
      "layer4.1.bn2.running_var \t torch.Size([512])\n",
      "layer4.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer4.1.conv3.weight \t torch.Size([2048, 512, 1, 1])\n",
      "layer4.1.bn3.weight \t torch.Size([2048])\n",
      "layer4.1.bn3.bias \t torch.Size([2048])\n",
      "layer4.1.bn3.running_mean \t torch.Size([2048])\n",
      "layer4.1.bn3.running_var \t torch.Size([2048])\n",
      "layer4.1.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer4.2.conv1.weight \t torch.Size([512, 2048, 1, 1])\n",
      "layer4.2.bn1.weight \t torch.Size([512])\n",
      "layer4.2.bn1.bias \t torch.Size([512])\n",
      "layer4.2.bn1.running_mean \t torch.Size([512])\n",
      "layer4.2.bn1.running_var \t torch.Size([512])\n",
      "layer4.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer4.2.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4.2.bn2.weight \t torch.Size([512])\n",
      "layer4.2.bn2.bias \t torch.Size([512])\n",
      "layer4.2.bn2.running_mean \t torch.Size([512])\n",
      "layer4.2.bn2.running_var \t torch.Size([512])\n",
      "layer4.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer4.2.conv3.weight \t torch.Size([2048, 512, 1, 1])\n",
      "layer4.2.bn3.weight \t torch.Size([2048])\n",
      "layer4.2.bn3.bias \t torch.Size([2048])\n",
      "layer4.2.bn3.running_mean \t torch.Size([2048])\n",
      "layer4.2.bn3.running_var \t torch.Size([2048])\n",
      "layer4.2.bn3.num_batches_tracked \t torch.Size([])\n",
      "fc.weight \t torch.Size([1000, 2048])\n",
      "fc.bias \t torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "\n",
    "model = torchvision.models.resnet152(pretrained  = False) #no internet to download weights\n",
    "#model.load_state_dict(torch.load('../input/pytorch-pretrained-models/resnet152-b121ed2d.pth')) #loading model from local\n",
    "\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer4 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plist = [\n",
    "         {'params': model.layer4.parameters(), 'lr': 1e-4, 'weight': 0.001},\n",
    "         {'params': model.fc.parameters(), 'lr': 1e-3}\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(model.layer4.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

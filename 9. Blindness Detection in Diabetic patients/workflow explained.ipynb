{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This **class RetinopathyDatasetTrain(Dataset):** has an unique way of loading images with **csv join to image folder**.\n",
    "- Also we have **image.resize** that can take some **data augumentation/data transformation parameters** like \n",
    "    - PIL.image.Nearest\n",
    "    - PIL.image.BILINEAR\n",
    "    - PIL.image.BICUBIC\n",
    "    - PIL.image.ANTIALIAS and for to know more click [here](https://pillow.readthedocs.io/en/3.0.x/releasenotes/2.7.0.html)\n",
    "- BILINEAR interploation with math [here](https://theailearner.com/2018/12/29/image-processing-bilinear-interpolation/)\n",
    "- image processing bilinear interpolation\n",
    "    - In computer vision and image processing, bilinear interpolation is one of the basic resampling techniques. In texture mapping, it is also known as bilinear filtering or bilinear texture mapping and can be used to produce a reasonably realistic image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview at EYE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/jpeg;base64,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\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "print(\"Overview at EYE\")\n",
    "Image(url= \"data:image/jpeg;base64,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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Bicubic and bilinear downscaling](https://pillow.readthedocs.io/en/3.0.x/releasenotes/2.7.0.html#bicubic-and-bilinear-downscaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the beginning **BILINEAR** and **BICUBIC** filters were based on affine transformations and used a fixed number of pixels from the source image for every destination pixel (2x2 pixels for **BILINEAR** and 4x4 for **BICUBIC**). This gave an unsatisfactory result for downscaling. At the same time, a high quality convolutions-based algorithm with flexible kernel was used for **ANTIALIAS** filter.\n",
    "\n",
    "Starting from Pillow 2.7.0, a high quality convolutions-based algorithm is used for all of these three filters.\n",
    "\n",
    "If you have previously used any tricks to maintain quality when downscaling with BILINEAR and BICUBIC filters (for example, reducing within several steps), they are unnecessary now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as this competition do not accept any internet access hence \n",
    "- load model from the folder,\n",
    "- and keep pretrained = False, \n",
    "    - if i keep pretrained == True , that would load imagenet weights.\n",
    "    - and we are using model from folder, where someone trained on their data.\n",
    "    - and we are using that model with weights on some other dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet101(pretrained=False)\n",
    "model.load_state_dict(torch.load(\"../input/pytorch-pretrained-models/resnet101-5d3b4d8f.pth\"))\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(2048, 1)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in **num_feautres = model.fc.in_features** takes no of input features of last fully connected layer in to argument called num_features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mode.fc** taking linear layer with 2048 input feautres and 1 output features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **why here is model.fc = nn.Linear(2048, 1) not model.fc = nn.Linear(num_features, 1)**?\n",
    "    - because num_features=2048 in resnet101."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading model to device with **model.to(device)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DAtaset+optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**batch_size  =16** and **shuffle= True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plist = [\n",
    "         {'params': model.layer4.parameters(), 'lr': 1e-4, 'weight': 0.001},\n",
    "         {'params': model.fc.parameters(), 'lr': 1e-3}\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in cell 4, the learning rates used are [0.001, 0.0001 + weight decay, 0.001]. I mean layer 4 has lower learning rate as compared to layer 2,3,1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(plist, lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "like here we did above \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sets the learning rate of each parameter** group to the initial lr decayed by gamma every step_size epochs. When by default last_epoch=-1, sets initial lr as lr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- optimizer (Optimizer) – Wrapped optimizer.\n",
    "\n",
    "- step_size (int) – Period of learning rate decay.\n",
    "\n",
    "- gamma (float) – Multiplicative factor of learning rate decay. Default: 0.1.\n",
    "\n",
    "- last_epoch (int) – The index of last epoch. Default: -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learn more [here](https://pytorch.org/docs/stable/optim.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stepLR\n",
    "- MultistepLR\n",
    "- ExponentialLR\n",
    "- CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean square error as loss here, because the metric is quadratic weighted kappa and He is treating it as **regression rather than classificataion.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WHY regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because instead of predicting the class, you predict the severity and from there chose the closest class.\n",
    "I think this works because 0 is the least severe and it gets more severe as you get to 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "since = time.time()\n",
    "criterion = nn.MSELoss()\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "    scheduler.step()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    tk0 = tqdm(data_loader, total=int(len(data_loader)))\n",
    "    counter = 0\n",
    "    for bi, d in enumerate(tk0):\n",
    "        inputs = d[\"image\"]\n",
    "        labels = d[\"labels\"].view(-1, 1)\n",
    "        inputs = inputs.to(device, dtype=torch.float)\n",
    "        labels = labels.to(device, dtype=torch.float)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        counter += 1\n",
    "        tk0.set_postfix(loss=(running_loss / (counter * data_loader.batch_size)))\n",
    "    epoch_loss = running_loss / len(data_loader)\n",
    "    print('Training Loss: {:.4f}'.format(epoch_loss))\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "torch.save(model.state_dict(), \"model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each epoch in 15 epochs\n",
    "- print \"epoch/15-1\"\n",
    "- print(\"*\"*50) in output every time it prints\n",
    "- scheduler.step() adjusts learning rate [here](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)\n",
    "    - **warning here**\n",
    "        - Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before the optimizer’s update; 1.1.0 changed this behavior in a BC-breaking way. If you use the learning rate scheduler (calling scheduler.step()) before the optimizer’s update (calling optimizer.step()), this will skip the first value of the learning rate schedule. If you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check if you are calling scheduler.step() at the wrong time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **model.train()** tells your model that you are training the model. So effectively layers like dropout, batchnorm etc. which behave different on the train and test procedures know what is going on and hence can behave accordingly.\n",
    "\n",
    "- More details: It sets the mode to train (see [source code](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.train)). You can call either model.eval() or model.train(mode=False) to tell that you are testing. It is somewhat intuitive to expect train function to train model but it does not do that. It just sets the mode.- explianed [here](https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initalizing running loss to 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change label shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = d[\"labels\"].view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set optimizer to zero grad first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set gradient to zero and then check the loss and backpropogate and update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.set_grad_enabled(True):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all done save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Adatptive avg pool 2d](https://pytorch.org/docs/stable/nn.html?highlight=adaptiveavgpool2d#torch.nn.AdaptiveAvgPool2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.avg_pool = nn.AdaptiveAvgPool2d(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applies a 2D adaptive average pooling over an input signal composed of several input planes.\n",
    "\n",
    "The output is of size H x W, for any input size. The number of output features is equal to the number of input planes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more about adaptive pooling [here](https://pytorch.org/docs/stable/nn.html?highlight=adaptiveavgpool2d#torch.nn.AdaptiveAvgPool2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> # target output size of 5x7\n",
    ">>> m = nn.AdaptiveAvgPool2d((5,7))\n",
    ">>> input = torch.randn(1, 64, 8, 9)\n",
    ">>> output = m(input)\n",
    ">>> # target output size of 7x7 (square)\n",
    ">>> m = nn.AdaptiveAvgPool2d(7)\n",
    ">>> input = torch.randn(1, 64, 10, 9)\n",
    ">>> output = m(input)\n",
    ">>> # target output size of 10x7\n",
    ">>> m = nn.AdaptiveMaxPool2d((None, 7))\n",
    ">>> input = torch.randn(1, 64, 10, 9)\n",
    ">>> output = m(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [What is adaptive avg pooling](https://discuss.pytorch.org/t/what-is-adaptiveavgpool2d/26897)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the specified output size is the output size, as in the documentation 991.\n",
    "\n",
    "In more detail:\n",
    "What happens is that the pooling stencil size (aka kernel size) is determined to be (input_size+target_size-1) // target_size, i.e. rounded up. With this Then the positions of where to apply the stencil are computed as rounded equidistant points between 0 and input_size - stencil_size.\n",
    "Let’s have a 1d example:\n",
    "Say you have an input size of 14 and a target size of 4. Then the stencil size is 4.\n",
    "The four equidistant points would be 0, 3.3333, 6.6666, 10 and get rounded to 0, 3, 7, 10. And so the four items would be the mean of the slices 0:4, 3:7, 7:11, 10:14 (in Python manner, so including lower bound, excluding upper bound). You see that the first two and last two slices overlap by one. Something like - occasional overlaps of 1 - this will generally be the case when the input size is not divisible by the target size.\n",
    "For experimentation, you could use arange and backward to see what happens. In the above toy example:\n",
    "\n",
    "a = torch.arange(0,14., requires_grad=True)\n",
    "b = torch.nn.functional.adaptive_avg_pool1d(a[None, None], 4)\n",
    "b.backward(torch.arange(1., 1+b.size(-1))[None,None])\n",
    "print (b, a.grad)\n",
    "Then b is 1.5, 4.5, 8.5, 11.5 just as you would get from slicing as above and taking the mean.\n",
    "The gradient a.grad shows the “receptive field of each output”:\n",
    "0.2500, 0.2500, 0.2500, 0.7500, 0.5000, 0.5000, 0.5000, 0.7500, 0.7500, 0.7500, 1.7500, 1.0000, 1.0000, 1.0000\n",
    "again, you see the overlap at item 3 and 10.\n",
    "\n",
    "Best regards\n",
    "\n",
    "Thomas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Ideas behind adaptive max pooling](https://forums.fast.ai/t/ideas-behind-adaptive-max-pooling/12634)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@alwc I think, Jeremy implies that he invented concatenation of Average and Max Pooling in the last layer. At the same time, someone else wrote a paper about it. And as far as the term “Adaptive” is concerned, it makes the following two lines of code equivalent:\n",
    "AvgPool2d(kernel_size=7, stride=7, padding=0) //Here the three parameters ensure that the output activation has 1by1 dimension\n",
    "nn.AdaptiveAvgPool2d(1) //Here we don’t specify the kernel_size, stride or padding. Instead, we specify the output dimension i.e 1by1\n",
    "\n",
    "Anyways, I still don’t know which paper discovered that concatenation of Avg and Max Pooling is better than Standalone Avg or Max Pooling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mentioning weights and input/output features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.last_linear = nn.Sequential(\n",
    "                          nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                          nn.Dropout(p=0.25),\n",
    "                          nn.Linear(in_features=2048, out_features=2048, bias=True),\n",
    "                          nn.ReLU(),\n",
    "                          nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                          nn.Dropout(p=0.5),\n",
    "                          nn.Linear(in_features=2048, out_features=1, bias=True),\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the model that was output of a kernel after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"../input/mmmodel/model.bin\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requires_grad = false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s a bit confusing, but model.train(False) doesn’t change param.requires_grad. It only changes the behavior of nn.Dropout and nn.BatchNorm (and maybe a few other modules) to use the inference-mode behavior. This disables the stochastic behavior of Dropout and using the running_mean/var in BatchNorm instead of batch statistics.\n",
    "\n",
    "If you want to freeze model weights, you should use the code snippet you wrote above:\n",
    "\n",
    "for param in model.parameters():\n",
    "     param.requires_grad = False\n",
    "Depending on how you want BatchNorm to behave, you may want to call model.train(False) or call it on some sub-module of your model.\n",
    "-- [here](https://discuss.pytorch.org/t/model-train-and-requires-grad/25845)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explained more clearly [here](https://stackoverflow.com/questions/51748138/pytorch-how-to-set-requires-grad-false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, they are the same. By default all the modules are initialized to train mode (self.training = True). Also be aware that some layers have different behavior during train/and evaluation (like BatchNorm, Dropout) so setting it matters.\n",
    "\n",
    "Also as a rule of thumb for programming in general, try to explicitly state your intent and set model.train() and model.eval() when necessary. --[here](https://discuss.pytorch.org/t/model-train-and-model-eval-vs-model-and-model-eval/5744)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More cleraly with code and everything explained [here](https://jamesmccaffrey.wordpress.com/2019/01/23/pytorch-train-vs-eval-mode/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout and BatchNorm (and maybe some custom modules) behave differently during training and evaluation. You must let the model know when to switch to eval mode by calling .eval() on the model.\n",
    "\n",
    "This sets self.training to False for every module in the model. If you are implementing your own module that must behave differently during training and evaluation, you can check the value of self.training while doing so.--[here](https://stackoverflow.com/questions/48146926/whats-the-meaning-of-function-eval-in-torch-nn-module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_preds1 = np.zeros((len(test_dataset), 1))\n",
    "tk0 = tqdm(test_data_loader)\n",
    "for i, x_batch in enumerate(tk0):\n",
    "    x_batch = x_batch[\"image\"]\n",
    "    pred = model(x_batch.to(device))\n",
    "    test_preds1[i * 32:(i + 1) * 32] = pred.detach().cpu().squeeze().numpy().ravel().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1952\n"
     ]
    }
   ],
   "source": [
    "print(61*32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which are test imageset size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing outputs keeping threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = [0.5, 1.5, 2.5, 3.5]\n",
    "\n",
    "for i, pred in enumerate(test_preds):\n",
    "    if pred < coef[0]:\n",
    "        test_preds[i] = 0\n",
    "    elif pred >= coef[0] and pred < coef[1]:\n",
    "        test_preds[i] = 1\n",
    "    elif pred >= coef[1] and pred < coef[2]:\n",
    "        test_preds[i] = 2\n",
    "    elif pred >= coef[2] and pred < coef[3]:\n",
    "        test_preds[i] = 3\n",
    "    else:\n",
    "        test_preds[i] = 4\n",
    "\n",
    "\n",
    "sample = pd.read_csv(\"../input/aptos2019-blindness-detection/sample_submission.csv\")\n",
    "sample.diagnosis = test_preds.astype(int)\n",
    "sample.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just clarifying, TTA is when you apply data augmentation to the test set and predict on it. Then you assign the prediction of each image to be the average prediction of every augmentation of that image.-- [here](https://www.kaggle.com/abhishek/pytorch-inference-kernel-lazy-tta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Data augmentation](https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/) is a technique often used to improve performance and reduce generalization error when training neural network models for computer vision problems.\n",
    "\n",
    "The image data augmentation technique can also be applied when making predictions with a fit model in order to allow the model to make predictions for multiple different versions of each image in the test dataset. The predictions on the augmented images can be averaged, which can result in better predictive performance.-- [here](https://machinelearningmastery.com/how-to-use-test-time-augmentation-to-improve-model-performance-for-image-classification/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes an ml model might get an image wrong, but if its flipped(or augmented in other ways) it might get it correct. The more augmented versions of the image given to the model to predict, the more likely it is to predict the correct class(i think)- from [here](https://www.kaggle.com/abhishek/pytorch-inference-kernel-lazy-tta) in comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# commetns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and Standev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### May I ask how you calculated the mean and std for normalization? :)\n",
    "\n",
    "[0.485, 0.456, 0.406], [0.229, 0.224, 0.225] are imagenet stats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### why rerunning may give different score, seeds?\n",
    "rotations are not deterministic.-- by Abhishek in comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It seems like the only augmentation performed here is a horizontal flip. Is that right? If so, why do you need 10 rounds of predictions?\n",
    "\n",
    "@lextoumbourou horizontal flip is performed randomly- by Abhishek in comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why coef = [0.5, 1.5, 2.5, 3.5]?\n",
    "The initial test_preds are continuous in this case, since Abhishek uses regression instead, so coef = [0.5, 1.5, 2.5, 3.5] is a list which includes the (initial) optimal thresholds needed for the final mapping to the discrete 0, 1, 2, 3 or 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
